{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "pyplot.ion()\n",
    "pyplot.style.use('dark_background')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn.functional as thf\n",
    "\n",
    "from rl.algs import environment\n",
    "from rl.algs import model\n",
    "from rl.algs import util\n",
    "\n",
    "\n",
    "\n",
    "Sars = environment.SARS\n",
    "\n",
    "class ReplayBuffer:\n",
    "  def __init__(self, size=10000):\n",
    "    self.max_size = size\n",
    "    self.buf = [None] * self.max_size\n",
    "    self.i = 0\n",
    "    self.size = 0\n",
    "\n",
    "  def add(self, entry):\n",
    "    self.buf[self.i] = entry\n",
    "    self.i = (self.i + 1) % self.max_size\n",
    "    if self.size < self.max_size:\n",
    "      self.size += 1\n",
    "\n",
    "  def sample(self, n=1):\n",
    "    \"\"\"Samples n entries uniformly at random, without replacement.\"\"\"\n",
    "    assert n <= self.size\n",
    "    idxs = np.random.choice(self.size, n, replace=False)\n",
    "    entries = [self.buf[i] for i in idxs]\n",
    "    return entries\n",
    "\n",
    "\n",
    "class Dqn:\n",
    "  def __init__(self, model, lr, eps_sched, target_update_freq=10000):\n",
    "    self.model = model\n",
    "    self.target_model = copy.deepcopy(self.model)\n",
    "    self.optimizer = th.optim.Adam(self.model.parameters(), lr)\n",
    "    self.eps_sched = eps_sched\n",
    "    self.target_update_freq = target_update_freq\n",
    "    self.gamma = 0.99\n",
    "    self.step = 0\n",
    "\n",
    "  def action_values(self, obs_np):\n",
    "    obs_var = util.to_variable(obs_np)\n",
    "    return self.model(obs_var)\n",
    "  \n",
    "  def get_action(self, obs_np):\n",
    "    qs = self.action_values(obs_np)\n",
    "    probs = thf.softmax(qs, dim=-1)\n",
    "    probs = util.to_numpy(probs)\n",
    "    return util.sample_eps_greedy(probs, self.eps_sched.get(self.step))\n",
    "\n",
    "  def _update_step_params(self):\n",
    "    self.step += 1\n",
    "    if self.step % self.target_update_freq == 0:\n",
    "      #print(f'step {self.step}: updating target model')\n",
    "      #print(f'eps: {self.eps_sched.get(self.step)}')\n",
    "      self.target_model = copy.deepcopy(self.model)\n",
    "\n",
    "  def update(self, sars_batch: List[Sars]):\n",
    "    metrics = {}\n",
    "    self._update_step_params()\n",
    "    bs = len(sars_batch)\n",
    "    empty = np.zeros_like(sars_batch[0].s)\n",
    "    s_batch = np.stack([sars.s for sars in sars_batch])\n",
    "    a_batch = np.stack([sars.a for sars in sars_batch])\n",
    "    r_batch = util.to_variable(np.stack([sars.r for sars in sars_batch]))\n",
    "    s1_batch = np.stack([sars.s1 if sars.s1 is not None else empty\n",
    "                         for sars in sars_batch])\n",
    "    \n",
    "    terminal_mask = util.to_variable(\n",
    "        np.stack([1 if sars.s1 is None else 0 for sars in sars_batch]))\n",
    "    \n",
    "    a_var = util.to_variable(a_batch, dtype=th.LongTensor).unsqueeze(dim=1)\n",
    "    \n",
    "    qs = self.action_values(s_batch)\n",
    "    qs_sel = th.gather(qs, dim=-1, index=a_var)\n",
    "    \n",
    "    s1_var = util.to_variable(s1_batch, volatile=True)\n",
    "    target_qs = self.target_model(s1_var)\n",
    "    target_qs_max, qs_max_idx = th.max(target_qs, dim=-1)\n",
    "    target = self.gamma * terminal_mask * target_qs_max\n",
    "    target += r_batch\n",
    "\n",
    "    loss = thf.mse_loss(qs_sel, target)\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    \n",
    "    metrics['loss'] = util.to_numpy(loss)[0]\n",
    "    return metrics\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 500; loss: 8.67045164341107e-05\n",
      "Step: 1000; loss: 0.00010578925866866484\n",
      "Step: 1500; loss: 9.354373469250277e-05\n",
      "Step: 2000; loss: 1.7775348169379868e-05\n",
      "Step: 2500; loss: 0.030549798160791397\n",
      "Step: 3000; loss: 0.00010811176616698503\n",
      "Step: 3500; loss: 0.031276166439056396\n",
      "Step: 4000; loss: 7.003571226960048e-06\n",
      "Step: 4500; loss: 3.901989839505404e-05\n",
      "Step: 5000; loss: 8.769686246523634e-06\n",
      "Step: 5500; loss: 4.7233999794116244e-05\n",
      "Step: 6000; loss: 4.62558591607376e-06\n",
      "Step: 6500; loss: 7.008868851698935e-05\n",
      "Step: 7000; loss: 3.2054704206530005e-05\n",
      "Step: 7500; loss: 0.00012739849626086652\n",
      "Step: 8000; loss: 3.5169457987649366e-05\n",
      "Step: 8500; loss: 1.694915226835292e-05\n",
      "Step: 9000; loss: 0.00017815487808547914\n",
      "Step: 9500; loss: 0.0002959122066386044\n",
      "Step: 10000; loss: 1.4976360944274347e-05\n",
      "Step: 10500; loss: 9.967227379092947e-05\n",
      "Step: 11000; loss: 0.0001555578492116183\n",
      "Step: 11500; loss: 0.03132355958223343\n",
      "Step: 12000; loss: 0.0004009358526673168\n",
      "Step: 12500; loss: 0.00011855692719109356\n",
      "Step: 13000; loss: 0.030566973611712456\n",
      "Step: 13500; loss: 0.03154769539833069\n",
      "Step: 14000; loss: 4.6928704250603914e-05\n",
      "Step: 14500; loss: 0.00027194456197321415\n",
      "Step: 15000; loss: 2.7690044589689933e-05\n",
      "Step: 15500; loss: 1.0650944204826374e-05\n",
      "Step: 16000; loss: 0.06046255677938461\n",
      "Step: 16500; loss: 9.511441749054939e-05\n",
      "Step: 17000; loss: 9.369426697958261e-05\n",
      "Step: 17500; loss: 2.4640175979584455e-05\n",
      "Step: 18000; loss: 1.7741878764354624e-05\n",
      "Step: 18500; loss: 0.00041134224738925695\n",
      "Step: 19000; loss: 2.3454107576981187e-05\n",
      "Step: 19500; loss: 0.4987918734550476\n",
      "Step: 20000; loss: 0.0002507427998352796\n",
      "Step: 20500; loss: 4.0805418393574655e-05\n",
      "Step: 21000; loss: 1.1487335541460197e-05\n",
      "Step: 21500; loss: 2.0339244656497613e-05\n",
      "Step: 22000; loss: 6.711110472679138e-05\n",
      "Step: 22500; loss: 5.702719136024825e-05\n",
      "Step: 23000; loss: 0.030520811676979065\n",
      "Step: 23500; loss: 0.00017782978829927742\n",
      "Step: 24000; loss: 0.031199371442198753\n",
      "Step: 24500; loss: 0.00022104602248873562\n",
      "Step: 25000; loss: 0.00014292278501670808\n",
      "Step: 25500; loss: 0.0002448251470923424\n",
      "Step: 26000; loss: 8.958540274761617e-05\n",
      "Step: 26500; loss: 0.03128534555435181\n",
      "Step: 27000; loss: 5.036892252974212e-05\n",
      "Step: 27500; loss: 0.0003975473518949002\n",
      "Step: 28000; loss: 3.3955824619624764e-05\n",
      "Step: 28500; loss: 2.1545922209043056e-05\n",
      "Step: 29000; loss: 2.1732988898293115e-05\n",
      "Step: 29500; loss: 0.00013200947432778776\n",
      "Step: 30000; loss: 5.463373236125335e-05\n",
      "Step: 30500; loss: 0.0008596176630817354\n",
      "Step: 31000; loss: 5.3694311645813286e-05\n",
      "Step: 31500; loss: 5.429469092632644e-05\n",
      "Step: 32000; loss: 0.0002520072739571333\n",
      "Step: 32500; loss: 9.94178481050767e-06\n",
      "Step: 33000; loss: 1.1558506230358034e-05\n",
      "Step: 33500; loss: 0.00024387620214838535\n",
      "Step: 34000; loss: 2.242761547677219e-05\n",
      "Step: 34500; loss: 7.10909953340888e-05\n",
      "Step: 35000; loss: 8.74395300343167e-06\n",
      "Step: 35500; loss: 0.00010392832336947322\n",
      "Step: 36000; loss: 2.6645597245078534e-05\n",
      "Step: 36500; loss: 0.00023248782963491976\n",
      "Step: 37000; loss: 0.000467927020508796\n",
      "Step: 37500; loss: 0.03126813843846321\n",
      "Step: 38000; loss: 4.685617022914812e-05\n",
      "Step: 38500; loss: 2.063130887108855e-05\n",
      "Step: 39000; loss: 0.00011509306204970926\n",
      "Step: 39500; loss: 2.0468700313358568e-05\n",
      "Step: 40000; loss: 0.00018682553491089493\n",
      "Step: 40500; loss: 0.00011146115139126778\n",
      "Step: 41000; loss: 0.00023445766419172287\n",
      "Step: 41500; loss: 0.00014890357851982117\n",
      "Step: 42000; loss: 6.373263022396713e-05\n",
      "Step: 42500; loss: 3.301796459709294e-05\n",
      "Step: 43000; loss: 1.9585488189477473e-05\n",
      "Step: 43500; loss: 0.00011207467468921095\n",
      "Step: 44000; loss: 5.2329382015159354e-05\n",
      "Step: 44500; loss: 0.00012356611841823906\n",
      "Step: 45000; loss: 0.00015454793174285442\n",
      "Step: 45500; loss: 5.759415580541827e-05\n",
      "Step: 46000; loss: 0.00031440300517715514\n",
      "Step: 46500; loss: 5.23744456586428e-05\n",
      "Step: 47000; loss: 0.0001133966216002591\n",
      "Step: 47500; loss: 4.825058113056002e-06\n",
      "Step: 48000; loss: 1.5118380360945594e-06\n",
      "Step: 48500; loss: 0.0003676271007861942\n",
      "Step: 49000; loss: 3.590679261833429e-05\n",
      "Step: 49500; loss: 3.583934449125081e-05\n",
      "Step: 50000; loss: 0.030867241322994232\n",
      "Step: 50500; loss: 5.2756764489458874e-05\n",
      "Step: 51000; loss: 0.00012759909441228956\n",
      "Step: 51500; loss: 0.030894435942173004\n",
      "Step: 52000; loss: 0.0001806039217626676\n",
      "Step: 52500; loss: 0.0001448242401238531\n",
      "Step: 53000; loss: 2.407608371868264e-05\n",
      "Step: 53500; loss: 0.00010394635319244117\n",
      "Step: 54000; loss: 2.2071391867939383e-05\n",
      "Step: 54500; loss: 4.447290120879188e-05\n",
      "Step: 55000; loss: 7.068232662277296e-05\n",
      "Step: 55500; loss: 2.443500761728501e-06\n",
      "Step: 56000; loss: 9.096969733946025e-06\n",
      "Step: 56500; loss: 0.0627884566783905\n",
      "Step: 57000; loss: 1.1516340236994438e-05\n",
      "Step: 57500; loss: 5.259227691567503e-05\n",
      "Step: 58000; loss: 6.127327196736587e-06\n",
      "Step: 58500; loss: 4.56880807178095e-05\n",
      "Step: 59000; loss: 5.0805232604034245e-05\n",
      "Step: 59500; loss: 0.061882875859737396\n",
      "Step: 60000; loss: 0.030514473095536232\n",
      "Step: 60500; loss: 1.5934938346617855e-05\n",
      "Step: 61000; loss: 0.00018381178961135447\n",
      "Step: 61500; loss: 2.4909491912694648e-05\n",
      "Step: 62000; loss: 4.17564224335365e-05\n",
      "Step: 62500; loss: 8.45281028887257e-05\n",
      "Step: 63000; loss: 0.030943958088755608\n",
      "Step: 63500; loss: 0.00015829663607291877\n",
      "Step: 64000; loss: 0.0308714397251606\n",
      "Step: 64500; loss: 4.45230798504781e-05\n",
      "Step: 65000; loss: 9.881273399514612e-06\n",
      "Step: 65500; loss: 2.1206142264418304e-06\n",
      "Step: 66000; loss: 0.00015855471428949386\n",
      "Step: 66500; loss: 2.4073502572719008e-05\n",
      "Step: 67000; loss: 0.00011593576346058398\n",
      "Step: 67500; loss: 9.461587069381494e-06\n",
      "Step: 68000; loss: 3.854741953546181e-05\n",
      "Step: 68500; loss: 9.627426334191114e-05\n",
      "Step: 69000; loss: 0.03121684305369854\n",
      "Step: 69500; loss: 9.197447070619091e-05\n",
      "Step: 70000; loss: 7.660016854060814e-05\n",
      "Step: 70500; loss: 0.00014605410979129374\n",
      "Step: 71000; loss: 2.594372199382633e-05\n",
      "Step: 71500; loss: 2.3840962967369705e-05\n",
      "Step: 72000; loss: 5.835920455865562e-05\n",
      "Step: 72500; loss: 4.259431443642825e-05\n",
      "Step: 73000; loss: 1.411751418345375e-05\n",
      "Step: 73500; loss: 1.4164816093398258e-06\n",
      "Step: 74000; loss: 0.0003930149250663817\n",
      "Step: 74500; loss: 0.0001783529296517372\n",
      "Step: 75000; loss: 3.88129883504007e-05\n",
      "Step: 75500; loss: 2.5213857952621765e-05\n",
      "Step: 76000; loss: 9.394869266543537e-06\n",
      "Step: 76500; loss: 4.162156255915761e-05\n",
      "Step: 77000; loss: 0.00019634372438304126\n",
      "Step: 77500; loss: 6.857966945972294e-05\n",
      "Step: 78000; loss: 0.00013627705629915\n",
      "Step: 78500; loss: 5.6233755458379164e-05\n",
      "Step: 79000; loss: 7.810165698174387e-06\n",
      "Step: 79500; loss: 5.3410207328852266e-05\n",
      "Step: 80000; loss: 4.698253178503364e-05\n",
      "Step: 80500; loss: 3.6973679016227834e-06\n",
      "Step: 81000; loss: 5.305189552018419e-05\n",
      "Step: 81500; loss: 4.431518027558923e-05\n",
      "Step: 82000; loss: 0.00017500929243396968\n",
      "Step: 82500; loss: 5.1609738875413314e-05\n",
      "Step: 83000; loss: 2.230472455266863e-05\n",
      "Step: 83500; loss: 0.030595999211072922\n",
      "Step: 84000; loss: 4.824613279197365e-05\n",
      "Step: 84500; loss: 1.920452996273525e-05\n",
      "Step: 85000; loss: 3.771115007111803e-05\n",
      "Step: 85500; loss: 3.0357696232385933e-06\n",
      "Step: 86000; loss: 3.140155604341999e-05\n",
      "Step: 86500; loss: 2.508025136194192e-05\n",
      "Step: 87000; loss: 5.326404425431974e-05\n",
      "Step: 87500; loss: 9.98943141894415e-06\n",
      "Step: 88000; loss: 0.0003596896422095597\n",
      "Step: 88500; loss: 1.91892177099362e-05\n",
      "Step: 89000; loss: 4.029340198030695e-05\n",
      "Step: 89500; loss: 3.7143540794204455e-06\n",
      "Step: 90000; loss: 2.6850957510760054e-05\n",
      "Step: 90500; loss: 6.808962098148186e-06\n",
      "Step: 91000; loss: 0.00025830153026618063\n",
      "Step: 91500; loss: 7.95359392213868e-06\n",
      "Step: 92000; loss: 9.184635018755216e-06\n",
      "Step: 92500; loss: 2.0529820176307112e-05\n",
      "Step: 93000; loss: 1.0309771823813207e-05\n",
      "Step: 93500; loss: 9.776180377230048e-06\n",
      "Step: 94000; loss: 0.0001331837847828865\n",
      "Step: 94500; loss: 4.360917955636978e-05\n",
      "Step: 95000; loss: 0.0001227288448717445\n",
      "Step: 95500; loss: 6.034609759808518e-05\n",
      "Step: 96000; loss: 3.230537913623266e-05\n",
      "Step: 96500; loss: 1.0070605640066788e-05\n",
      "Step: 97000; loss: 0.00012077848805347458\n",
      "Step: 97500; loss: 9.535548451822251e-05\n",
      "Step: 98000; loss: 0.0002847032737918198\n",
      "Step: 98500; loss: 6.875243707327172e-05\n",
      "Step: 99000; loss: 8.720453479327261e-05\n",
      "Step: 99500; loss: 0.00018534623086452484\n",
      "Step: 100000; loss: 1.3584749467554502e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100500; loss: 0.030735371634364128\n",
      "Step: 101000; loss: 0.0002753920271061361\n",
      "Step: 101500; loss: 2.6325738872401416e-05\n",
      "Step: 102000; loss: 9.550950198899955e-05\n",
      "Step: 102500; loss: 9.838803089223802e-05\n",
      "Step: 103000; loss: 1.9119990611216053e-05\n",
      "Step: 103500; loss: 1.6171796232811175e-05\n",
      "Step: 104000; loss: 6.72093010507524e-05\n",
      "Step: 104500; loss: 0.00017348774417769164\n",
      "Step: 105000; loss: 0.03145001828670502\n",
      "Step: 105500; loss: 0.00010920783097390085\n",
      "Step: 106000; loss: 1.4329498299048282e-05\n",
      "Step: 106500; loss: 0.03164493665099144\n",
      "Step: 107000; loss: 5.766302638221532e-05\n",
      "Step: 107500; loss: 0.03127219155430794\n",
      "Step: 108000; loss: 5.597872586804442e-06\n",
      "Step: 108500; loss: 5.353752931114286e-05\n",
      "Step: 109000; loss: 0.0312635712325573\n",
      "Step: 109500; loss: 0.03060961328446865\n",
      "Step: 110000; loss: 6.675773875031155e-06\n",
      "Step: 110500; loss: 0.0630304142832756\n",
      "Step: 111000; loss: 7.335822738241404e-05\n",
      "Step: 111500; loss: 0.031244652345776558\n",
      "Step: 112000; loss: 8.348397386725992e-05\n",
      "Step: 112500; loss: 4.183818236924708e-05\n",
      "Step: 113000; loss: 4.339254155638628e-05\n",
      "Step: 113500; loss: 6.824098818469793e-05\n",
      "Step: 114000; loss: 4.371823160909116e-05\n",
      "Step: 114500; loss: 1.578196133777965e-05\n",
      "Step: 115000; loss: 4.105537664145231e-05\n",
      "Step: 115500; loss: 4.482571966946125e-05\n",
      "Step: 116000; loss: 6.067106733098626e-05\n",
      "Step: 116500; loss: 6.139964625617722e-07\n",
      "Step: 117000; loss: 3.82786565751303e-05\n",
      "Step: 117500; loss: 0.03073517233133316\n",
      "Step: 118000; loss: 4.5208787923911586e-05\n",
      "Step: 118500; loss: 0.00017210401711054146\n",
      "Step: 119000; loss: 0.03043879196047783\n",
      "Step: 119500; loss: 0.062441349029541016\n",
      "Step: 120000; loss: 2.8400438168318942e-05\n",
      "Step: 120500; loss: 0.03078627958893776\n",
      "Step: 121000; loss: 7.519192149629816e-05\n",
      "Step: 121500; loss: 0.00013088685227558017\n",
      "Step: 122000; loss: 0.00020087182929273695\n",
      "Step: 122500; loss: 4.965774860465899e-05\n",
      "Step: 123000; loss: 7.530972652602941e-05\n",
      "Step: 123500; loss: 7.488715345971286e-05\n",
      "Step: 124000; loss: 0.00024410782498307526\n",
      "Step: 124500; loss: 2.325874629605096e-05\n",
      "Step: 125000; loss: 3.2096708309836686e-05\n",
      "Step: 125500; loss: 2.1627552371228376e-07\n",
      "Step: 126000; loss: 1.952850288944319e-05\n",
      "Step: 126500; loss: 5.127137774252333e-05\n",
      "Step: 127000; loss: 6.15780518273823e-05\n",
      "Step: 127500; loss: 8.480970791424625e-06\n",
      "Step: 128000; loss: 9.827047324506566e-05\n",
      "Step: 128500; loss: 3.4443844469933538e-06\n",
      "Step: 129000; loss: 0.03148015961050987\n",
      "Step: 129500; loss: 1.6946985851973295e-05\n",
      "Step: 130000; loss: 4.387113676784793e-06\n",
      "Step: 130500; loss: 6.089869566494599e-05\n",
      "Step: 131000; loss: 5.518819307326339e-05\n",
      "Step: 131500; loss: 0.03125842660665512\n",
      "Step: 132000; loss: 8.100549166556448e-05\n",
      "Step: 132500; loss: 4.828457531402819e-05\n",
      "Step: 133000; loss: 4.942330269841477e-05\n",
      "Step: 133500; loss: 3.297719104011776e-06\n",
      "Step: 134000; loss: 2.5041395929292776e-05\n",
      "Step: 134500; loss: 7.576313237223076e-06\n",
      "Step: 135000; loss: 9.211687574861571e-05\n",
      "Step: 135500; loss: 8.288503886433318e-05\n",
      "Step: 136000; loss: 3.468077920842916e-05\n",
      "Step: 136500; loss: 7.136971544241533e-05\n",
      "Step: 137000; loss: 0.00017654162365943193\n",
      "Step: 137500; loss: 0.00014952235505916178\n",
      "Step: 138000; loss: 1.3936812592874048e-06\n",
      "Step: 138500; loss: 8.72219152370235e-06\n",
      "Step: 139000; loss: 0.03040163218975067\n",
      "Step: 139500; loss: 0.00019100846839137375\n",
      "Step: 140000; loss: 7.349297902692342e-06\n",
      "Step: 140500; loss: 5.5161195632535964e-05\n",
      "Step: 141000; loss: 6.257537461351603e-05\n",
      "Step: 141500; loss: 0.03069404326379299\n",
      "Step: 142000; loss: 0.00016272146604023874\n",
      "Step: 142500; loss: 3.503925199765945e-06\n",
      "Step: 143000; loss: 3.929891317966394e-05\n",
      "Step: 143500; loss: 0.00017837833729572594\n",
      "Step: 144000; loss: 1.7682028556009755e-05\n",
      "Step: 144500; loss: 0.00021617283346131444\n",
      "Step: 145000; loss: 2.8039496100973338e-05\n",
      "Step: 145500; loss: 1.8124860616808292e-06\n",
      "Step: 146000; loss: 1.5427733160322532e-05\n",
      "Step: 146500; loss: 4.4224630983080715e-05\n",
      "Step: 147000; loss: 0.03130354732275009\n",
      "Step: 147500; loss: 6.618066254304722e-05\n",
      "Step: 148000; loss: 0.031336866319179535\n",
      "Step: 148500; loss: 4.4409964175429195e-05\n",
      "Step: 149000; loss: 2.4205171939684078e-05\n",
      "Step: 149500; loss: 0.00012862250150647014\n",
      "Step: 150000; loss: 0.00013714715896639973\n",
      "Step: 150500; loss: 0.00017581276188138872\n",
      "Step: 151000; loss: 0.0003300796961411834\n",
      "Step: 151500; loss: 0.00018219079356640577\n",
      "Step: 152000; loss: 2.9252110834931955e-05\n",
      "Step: 152500; loss: 4.3953747081104666e-05\n",
      "Step: 153000; loss: 3.2005955290514976e-05\n",
      "Step: 153500; loss: 0.00016296448302455246\n",
      "Step: 154000; loss: 6.131184636615217e-05\n",
      "Step: 154500; loss: 7.736254337942228e-05\n",
      "Step: 155000; loss: 0.030890483409166336\n",
      "Step: 155500; loss: 0.030514847487211227\n",
      "Step: 156000; loss: 0.030819499865174294\n",
      "Step: 156500; loss: 2.344114000152331e-05\n",
      "Step: 157000; loss: 3.072666731895879e-05\n",
      "Step: 157500; loss: 9.738569497130811e-05\n",
      "Step: 158000; loss: 2.5026616640388966e-05\n",
      "Step: 158500; loss: 4.566587813314982e-05\n",
      "Step: 159000; loss: 0.00050235481467098\n",
      "Step: 159500; loss: 0.00010321539593860507\n",
      "Step: 160000; loss: 6.629214112763293e-06\n",
      "Step: 160500; loss: 0.03129637613892555\n",
      "Step: 161000; loss: 2.3264449282578425e-06\n",
      "Step: 161500; loss: 0.0001690350181888789\n",
      "Step: 162000; loss: 5.08575867570471e-05\n",
      "Step: 162500; loss: 3.3587971302040387e-06\n",
      "Step: 163000; loss: 0.00015195811283774674\n",
      "Step: 163500; loss: 3.852916051982902e-06\n",
      "Step: 164000; loss: 6.453596142819151e-05\n",
      "Step: 164500; loss: 9.362504351884127e-05\n",
      "Step: 165000; loss: 6.800634582759812e-05\n",
      "Step: 165500; loss: 0.03128983825445175\n",
      "Step: 166000; loss: 0.00022272663773037493\n",
      "Step: 166500; loss: 2.487595338607207e-05\n",
      "Step: 167000; loss: 8.253807754954323e-05\n",
      "Step: 167500; loss: 1.2447579138097353e-05\n",
      "Step: 168000; loss: 8.580785106460098e-06\n",
      "Step: 168500; loss: 0.031237183138728142\n",
      "Step: 169000; loss: 7.3654741754580755e-06\n",
      "Step: 169500; loss: 0.030924545601010323\n",
      "Step: 170000; loss: 3.075820131925866e-05\n",
      "Step: 170500; loss: 9.1660360340029e-05\n",
      "Step: 171000; loss: 6.082702384446748e-06\n",
      "Step: 171500; loss: 0.030368557199835777\n",
      "Step: 172000; loss: 3.2159093734662747e-06\n",
      "Step: 172500; loss: 1.486975270381663e-05\n",
      "Step: 173000; loss: 7.528549758717418e-05\n",
      "Step: 173500; loss: 4.639138569473289e-05\n",
      "Step: 174000; loss: 7.315885341085959e-06\n",
      "Step: 174500; loss: 5.992857040837407e-06\n",
      "Step: 175000; loss: 1.7171674699056894e-05\n",
      "Step: 175500; loss: 0.00012177332246210426\n",
      "Step: 176000; loss: 0.00011197334970347583\n",
      "Step: 176500; loss: 0.031161364167928696\n",
      "Step: 177000; loss: 3.40798324032221e-05\n",
      "Step: 177500; loss: 0.00015607860404998064\n",
      "Step: 178000; loss: 0.00023326557129621506\n",
      "Step: 178500; loss: 3.834623930742964e-05\n",
      "Step: 179000; loss: 0.00013090824359096587\n",
      "Step: 179500; loss: 0.00018186996749136597\n",
      "Step: 180000; loss: 2.445004429318942e-05\n",
      "Step: 180500; loss: 0.00013687317550648004\n",
      "Step: 181000; loss: 4.070098657393828e-05\n",
      "Step: 181500; loss: 1.704352325759828e-05\n",
      "Step: 182000; loss: 6.469694199040532e-05\n",
      "Step: 182500; loss: 0.000213604886084795\n",
      "Step: 183000; loss: 9.599221812095493e-06\n",
      "Step: 183500; loss: 0.00016140402294695377\n",
      "Step: 184000; loss: 0.000111591667518951\n",
      "Step: 184500; loss: 2.6821106075658463e-05\n",
      "Step: 185000; loss: 0.0001519202342024073\n",
      "Step: 185500; loss: 0.03079642914235592\n",
      "Step: 186000; loss: 7.33253255020827e-05\n",
      "Step: 186500; loss: 0.00011954227375099435\n",
      "Step: 187000; loss: 5.20716785104014e-05\n",
      "Step: 187500; loss: 1.024086850520689e-05\n",
      "Step: 188000; loss: 5.7595574617153034e-05\n",
      "Step: 188500; loss: 2.8735492378473282e-05\n",
      "Step: 189000; loss: 1.095075458579231e-05\n",
      "Step: 189500; loss: 2.5691688279039226e-05\n",
      "Step: 190000; loss: 2.6371752028353512e-05\n",
      "Step: 190500; loss: 2.4439236767648254e-06\n",
      "Step: 191000; loss: 8.197658462449908e-05\n",
      "Step: 191500; loss: 0.06230154633522034\n",
      "Step: 192000; loss: 0.030666522681713104\n",
      "Step: 192500; loss: 4.332284879637882e-05\n",
      "Step: 193000; loss: 3.20250510412734e-05\n",
      "Step: 193500; loss: 3.425469185458496e-05\n",
      "Step: 194000; loss: 0.032018065452575684\n",
      "Step: 194500; loss: 3.599206684157252e-05\n",
      "Step: 195000; loss: 5.109210542286746e-05\n",
      "Step: 195500; loss: 0.0002689231187105179\n",
      "Step: 196000; loss: 3.1890955142444e-05\n",
      "Step: 196500; loss: 1.932031591422856e-05\n",
      "Step: 197000; loss: 1.9819075532723218e-05\n",
      "Step: 197500; loss: 5.319442061590962e-05\n",
      "Step: 198000; loss: 0.030761756002902985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 198500; loss: 4.381549661047757e-05\n",
      "Step: 199000; loss: 9.507096365268808e-06\n",
      "Step: 199500; loss: 0.00011966974125243723\n"
     ]
    }
   ],
   "source": [
    "# Train the DQN\n",
    "\n",
    "from rl.algs import experiment\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_START_STEP = 100\n",
    "NUM_TRAIN_STEPS = 200000\n",
    "REPLAY_SIZE = 5000\n",
    "\n",
    "\n",
    "env = environment.AtariEnvironment('Breakout')\n",
    "model_params = {\n",
    "}\n",
    "policy_params = {\n",
    "    'lr': 0.005,\n",
    "    'eps_sched': util.Schedule((0, TRAIN_START_STEP, NUM_TRAIN_STEPS),\n",
    "                               (1, 1, 0.1)),\n",
    "    'target_update_freq': 1000,\n",
    "}\n",
    "exp = experiment.Experiment(env, model.QNetwork, model_params,\n",
    "                            Dqn, policy_params)\n",
    "\n",
    "\n",
    "rb = ReplayBuffer(REPLAY_SIZE)\n",
    "\n",
    "for step in range(TRAIN_START_STEP):\n",
    "  sars = env.step(exp.policy.get_action)\n",
    "  rb.add(sars)\n",
    "\n",
    "for step in range(TRAIN_START_STEP, NUM_TRAIN_STEPS):\n",
    "  sars = env.step(exp.policy.get_action)\n",
    "  rb.add(sars)\n",
    "  experience_batch = rb.sample(BATCH_SIZE)\n",
    "  m = exp.policy.update(experience_batch)\n",
    "  if step % 500 == 0:\n",
    "    print(f'Step: {step}; loss: {m[\"loss\"]}')\n",
    "    \n",
    "def debug():\n",
    "  for i in range(4):\n",
    "    pyplot.figure()\n",
    "    pyplot.imshow(env.last_obs[i, :, :], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.visualize(exp.policy, steps=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 3 4 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff5e5f60198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.random.choice(5, 5, replace=False))\n",
    "def show(sars):\n",
    "  pyplot.figure()\n",
    "  pyplot.imshow(sars.s[0, :, :], cmap='gray')\n",
    "\n",
    "show(rb.sample()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = Dqn()\n",
    "dqn.get_action(np.random.rand(210))\n",
    "\n",
    "test_sars = [\n",
    "    Sars(np.random.rand(210), 3, 0, np.random.rand(210)),\n",
    "    Sars(np.random.rand(210), 0, 1, None),\n",
    "    Sars(np.random.rand(210), 2, 2, np.random.rand(210)),\n",
    "    Sars(np.random.rand(210), 1, 3, None)\n",
    "]\n",
    "dqn.update(test_sars)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f', 'c']\n",
      "array([[0.54102279, 0.90586718, 0.33795042, 0.78032276],\n",
      "       [0.30351326, 0.70604155, 0.98818081, 0.04975847],\n",
      "       [0.43589021, 0.99924586, 0.77265579, 0.15717228],\n",
      "       [0.85018242, 0.05662995, 0.83276985, 0.37340199],\n",
      "       [0.07857284, 0.86944281, 0.60107863, 0.9752491 ]])\n",
      "array([0, 2, 1, 3, 0])\n",
      "array([0.54102279, 0.98818081, 0.99924586, 0.37340199, 0.07857284])\n",
      "array([[0.54102279, 0.90586718, 0.33795042, 0.78032276],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.85018242, 0.05662995, 0.83276985, 0.37340199],\n",
      "       [0.        , 0.        , 0.        , 0.        ]])\n"
     ]
    }
   ],
   "source": [
    "rb = ReplayBuffer(5)\n",
    "[rb.add(c) for c in 'abcdef']\n",
    "print(rb.sample(2))\n",
    "\n",
    "qs = np.random.rand(5, 4)\n",
    "actions = np.array([0, 2, 1, 3, 0], dtype=np.int)\n",
    "pprint(qs)\n",
    "pprint(actions)\n",
    "pprint(qs[range(5), actions])\n",
    "qs[[False, True, True, False, True], :] = 0\n",
    "pprint(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0.5917257 , 0.50399713, 0.35883697],\n",
      "       [0.32559733, 0.71055613, 0.83439692]]),\n",
      " array([[0.60927262, 0.2183909 , 0.34192333],\n",
      "       [0.03653836, 0.81768947, 0.4716851 ]]),\n",
      " array([[0.60927262, 0.50399713, 0.35883697],\n",
      "       [0.32559733, 0.81768947, 0.83439692]]))\n"
     ]
    }
   ],
   "source": [
    "from skimage import color\n",
    "rgb = np.zeros((2, 1, 3))\n",
    "rgb[1, 0, :] = 1\n",
    "color.rgb2yuv(rgb)[:, :, 0]\n",
    "x, y = np.random.rand(2, 3), np.random.rand(2, 3)\n",
    "pprint((x, y, np.maximum(x, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rl.algs import util\n",
    "\n",
    "s = util.Schedule((0, 100, 100, 200), (1, 1, 0.5, 0))\n",
    "s.get(150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
