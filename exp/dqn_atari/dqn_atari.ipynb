{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\t Loss: 2.317120\n",
      "Test set: Average loss: 0.0914, Accuracy: 9718/10000 (97.18%)\n",
      "Train Epoch: 2 [0/60000 (0%)]\t Loss: 0.129938\n",
      "Test set: Average loss: 0.0653, Accuracy: 9801/10000 (98.01%)\n",
      "Train Epoch: 3 [0/60000 (0%)]\t Loss: 0.053309\n",
      "Test set: Average loss: 0.0560, Accuracy: 9819/10000 (98.19%)\n",
      "Train Epoch: 4 [0/60000 (0%)]\t Loss: 0.027960\n",
      "Test set: Average loss: 0.0549, Accuracy: 9810/10000 (98.10%)\n",
      "Train Epoch: 5 [0/60000 (0%)]\t Loss: 0.047343\n",
      "Test set: Average loss: 0.0455, Accuracy: 9860/10000 (98.60%)\n",
      "Train Epoch: 6 [0/60000 (0%)]\t Loss: 0.020113\n",
      "Test set: Average loss: 0.0405, Accuracy: 9861/10000 (98.61%)\n",
      "Train Epoch: 7 [0/60000 (0%)]\t Loss: 0.009371\n",
      "Test set: Average loss: 0.0400, Accuracy: 9867/10000 (98.67%)\n",
      "Train Epoch: 8 [0/60000 (0%)]\t Loss: 0.054427\n",
      "Test set: Average loss: 0.0347, Accuracy: 9885/10000 (98.85%)\n",
      "Train Epoch: 9 [0/60000 (0%)]\t Loss: 0.171189\n",
      "Test set: Average loss: 0.0364, Accuracy: 9876/10000 (98.76%)\n",
      "Train Epoch: 10 [0/60000 (0%)]\t Loss: 0.025680\n",
      "Test set: Average loss: 0.0300, Accuracy: 9896/10000 (98.96%)\n"
     ]
    }
   ],
   "source": [
    "# Toy mnist dataset\n",
    "\n",
    "def get_loader(train):\n",
    "  mnist = datasets.MNIST(\n",
    "      'data', train=train, download=True,\n",
    "      transform=transforms.Compose([\n",
    "          transforms.ToTensor(),\n",
    "          transforms.Normalize((0.1307,), (0.3081,))\n",
    "      ]))\n",
    "  #plt.imshow(mnist.__getitem__(0)[0])\n",
    "  return torch.utils.data.DataLoader(\n",
    "      mnist, batch_size=64, shuffle=True,\n",
    "      num_workers=4)\n",
    "train_loader = get_loader(train=True)\n",
    "test_loader = get_loader(train=False)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  def __init__(self, in_d, out_d):\n",
    "    super(CNN, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_d, 16, kernel_size=5)\n",
    "    self.conv2 = nn.Conv2d(16, 32, kernel_size=5)\n",
    "    self.fc = nn.Linear(32 * 4 * 4, out_d)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "    x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "    x = self.fc(x.view(x.size(0), -1))\n",
    "    return F.log_softmax(x)\n",
    "\n",
    "model = CNN(in_d=1, out_d=10)\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01,\n",
    "                      momentum=0.5)\n",
    "\n",
    "def train(epoch):\n",
    "  model.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % 1000 == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'. format(\n",
    "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "          100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  for data, target in test_loader:\n",
    "    data, target = data.cuda(), target.cuda()\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    output = model(data)\n",
    "    test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "    pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "      test_loss, correct, len(test_loader.dataset),\n",
    "      100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "  train(epoch)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN (\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc): Linear (6272 -> 10)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
